{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RzElOuzAOjG"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/torresmateo/penguin-tf-workshop/blob/master/D3_1_Natural_Language.ipynb\" target=\"_parent\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Procesamiento de Lenguaje Natural\n",
    "(Natural Language Processing)\n",
    "\n",
    "En este notebook se muestran ejemplos de como lidiar con datos de texto. A diferencia de las imágenes, un texto no se puede representar numéricamente como la intensidad de una coordenada en una matriz bidimensional. Pero aún podemos hacer una representación (o **codificación**) de un texto usando números. \n",
    "\n",
    "TensorFlow nos ofrece varias funcionalidades para ello.\n",
    "\n",
    "Antes que nada, importamos las bibliotecas necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Lh0ntq-qoOMB",
    "outputId": "6adf3a0c-fc02-4cba-ddf6-f117f2377ff4"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww_eNS9KDchs"
   },
   "source": [
    "# Tokens\n",
    "\n",
    "En un **corpus** de texto, existe siempre un conjunto finito de palabras. Esto es el **vocabulario** de ese corpus. Una forma muy tradicional de codificar el texto de forma numérica es asignar a cada palabra un **índice** numérico que nos indique la palabra. Para ello, necesitamos una lista ordenada con dicho índice.\n",
    "\n",
    "Programar esto por nuestra cuenta no parece demasiado complicado, pero hay muchos casos en que hay que tomar decisiones importantes. Por ejemplo:\n",
    "* Si tenemos memoria limitada y sólo podemos guardar 2000 palabras, ¿cuáles palabras del vocabulario completo debemos ignorar? \n",
    "* ¿Qué hacemos con las palabras ignoradas?\n",
    "\n",
    "Abajo vemos el código que se usa para **tokenizar** un texto (codificar de forma numérica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "bSAn23FXDbv8",
    "outputId": "78b1571a-e317-41bc-9dea-6a38e840e9c2"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'había una vez',\n",
    "    'un gato montes',\n",
    "    'que tenía la cola al revés',\n",
    "    'querés que te cuente otra vez?'\n",
    "]\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4s7v3bHGnYQ"
   },
   "source": [
    "podemos ver que nuestro **vocabulario** es un diccionario que nos dará el índice numérico para cada palabra del corpus. En el caso de arriba, tenemos menos palabras en el corpus de las configuradas en el *tokenizer*. \n",
    "\n",
    "Una vez identificado el vocabulario, podemos **codificar** nuestro corpus en secuencias numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Nw5R8YUmapng",
    "outputId": "41fbc7f4-4014-4e37-9b87-6ecee1e94439"
   },
   "outputs": [],
   "source": [
    "secuencias = tokenizer.texts_to_sequences(corpus)\n",
    "print(secuencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9nRKzuUbh0F"
   },
   "source": [
    "A partir de la secuencia, desde luego podemos volver al texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Mufl2ULfCK-",
    "outputId": "4359dc34-61f0-4db8-d188-1c864f9d51bc"
   },
   "outputs": [],
   "source": [
    "texto = tokenizer.sequences_to_texts(secuencias)\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj1jHXHvfOHh"
   },
   "source": [
    "Note como hemos perdido el signo de puntuación final. Esto se debe a que entre los parámetros del [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) hay un filtro, que nos permite controlar qué simbolos se ignoran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oQylDvmapPS"
   },
   "source": [
    "# Valores fuera del vocabulario\n",
    "\n",
    "Hay muchos motivos por los cuales al analizar un texto pueda aparecer una palabra que no se encuentra en nuestro vocabulario:\n",
    "\n",
    "* Nuestro corpus no era demasiado variado en cuanto a palabras se refiere.\n",
    "* La nueva palabra está incorrectamente escrita.\n",
    "* La nueva palabra está en otro idioma.\n",
    "\n",
    "Para lidiar con estos casos, podemos usar un parámetro del Tokenizer que nos permite definir un `str` para los casos fuera de vocabulario (out of vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "js-8ye2hGEu-",
    "outputId": "4f41f662-e442-4dbe-b367-591c6a80e1d5"
   },
   "outputs": [],
   "source": [
    "# crear un tokenizer, limitando la cantidad de palabras\n",
    "tokenizer_oov = tf.keras.preprocessing.text.Tokenizer(num_words = 100, oov_token=\"<???>\")\n",
    "\n",
    "tokenizer_oov.fit_on_texts(corpus)\n",
    "word_index = tokenizer_oov.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLUy6rJniHKu"
   },
   "source": [
    "Note como el valor que definimos se usa como primer elemento del diccionario.\n",
    "\n",
    "Veamos que pasa cuando intentamos codificar una oración con una palabra afuera del diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5__ay-6gJJPi",
    "outputId": "40f46280-6fb5-4e2f-dae2-69db3f203ed5"
   },
   "outputs": [],
   "source": [
    "# agregamos una palabra al corpus original\n",
    "corpus_oov = corpus + ['Oración con palábras nunca antes vistas']\n",
    "\n",
    "# usamos el tokenizador para obtener las secuencias\n",
    "secuencias_oov = tokenizer_oov.texts_to_sequences(corpus_oov)\n",
    "print(secuencias_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05-hPTs_jIwY"
   },
   "source": [
    "Observamos como la última secuencia esta formada solo de palábras que no existen en nuestro vocabulario. Claramente, al intentar recuperar el texto, solo podemos hacerlo parcialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rly-PTi0MSJG",
    "outputId": "d7a6396c-8678-4e31-cf2b-ed7e90697860"
   },
   "outputs": [],
   "source": [
    "texto_oov = tokenizer_oov.sequences_to_texts(secuencias_oov)\n",
    "print(texto_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bjm4YrKLjXWZ",
    "outputId": "6c8b8bb8-c06d-4ef8-9689-c366a5e89432"
   },
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(corpus_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FD9cZ0ZjeXE"
   },
   "source": [
    "# Estandarizar las secuencias\n",
    "\n",
    "Un problema a la hora de usar oraciones es que su longitud es variable, veamos la longitud de cada oración de nuestro corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JwRZ0CpFja7I",
    "outputId": "f7949b8d-6c7a-4a25-d647-624c48a1c8a6"
   },
   "outputs": [],
   "source": [
    "for oracion in corpus:\n",
    "    print(len(oracion.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGz9oaRMj2Wu"
   },
   "source": [
    "Una de las secuencias es más larga que las demás. Para lidiar con este problma, podemos seguir varias estrategias, pero por lo general se decide hacer \"padding\". Esto funciona en dos posibles modos:\n",
    "\n",
    "* **pre**: va a agregar ceros ANTES de las secuencias cortas\n",
    "* **post**: va a agregar ceros DESPUES de las secuencias cortas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "KQm2uI0Ojxub",
    "outputId": "bf19a1ba-f396-47ba-9e8f-880e27fe0dfa"
   },
   "outputs": [],
   "source": [
    "padded = pad_sequences(secuencias, padding='pre')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mSJQTNrknHI"
   },
   "source": [
    "Veamos que texto es equivalente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1edw11m9klAf",
    "outputId": "949c2cce-1885-496b-f20c-59535c74957a"
   },
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvrvmbF8kWQR"
   },
   "source": [
    "En muchas ocasiones, puede ocurrir que la oración más larga es mucho más larga que lo razonable para nuestro modelo, y las secuencias generadas serían mayoritariamente cero. la funcíon `pad_sequences` soporta un parámetro `maxlen`, que nos sirve de ayuda en estos casos, limitando la longitud de la salida. Para controlar como se cmporta este parámetro (si va a eliminar las palabras desde el comienzo o el final de la oración, podemos definir el parámetro `truncating`). Siempre es bueno tener la  [documentación](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) a mano para entender lo que la función hace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oSrYQtPqjAM_",
    "outputId": "f41f2eb4-c142-4e26-8666-0d71437e1ec2"
   },
   "outputs": [],
   "source": [
    "padded = pad_sequences(secuencias, padding='pre', maxlen=4)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2vkk2KCkwa6"
   },
   "source": [
    "Se debe estudiar en que casos es preferible esto en vez de eliminar la oración del dataset. Ya que se pierde gran parte del texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C25p-aobk4ki",
    "outputId": "b06bb884-5e6f-457a-aeee-8ec1e28718a8"
   },
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shAeIqQcjBXS"
   },
   "source": [
    "# Créditos \n",
    "\n",
    "Este notebook utiliza y modifica contenido del curso online [TensorFlow in Practice](https://www.deeplearning.ai/tensorflow-in-practice/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "D3_1_Natural_Language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
